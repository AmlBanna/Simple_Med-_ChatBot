{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6966bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Collecting pdfplumber\n",
      "  Obtaining dependency information for pdfplumber from https://files.pythonhosted.org/packages/e6/c4/d2e09fbc937d1f76baae34e526662cc718e23a904321bf4a40282d190033/pdfplumber-0.11.6-py3-none-any.whl.metadata\n",
      "  Downloading pdfplumber-0.11.6-py3-none-any.whl.metadata (42 kB)\n",
      "     ---------------------------------------- 0.0/42.8 kB ? eta -:--:--\n",
      "     ------------------ ------------------- 20.5/42.8 kB 330.3 kB/s eta 0:00:01\n",
      "     -------------------------------------- 42.8/42.8 kB 524.4 kB/s eta 0:00:00\n",
      "Collecting pdfminer.six==20250327 (from pdfplumber)\n",
      "  Obtaining dependency information for pdfminer.six==20250327 from https://files.pythonhosted.org/packages/29/2f/409e174b5a0195aa6a814c7359a1285f1c887a4c84aff17ed03f607c06ba/pdfminer_six-20250327-py3-none-any.whl.metadata\n",
      "  Downloading pdfminer_six-20250327-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from pdfplumber) (9.4.0)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Obtaining dependency information for pypdfium2>=4.18.0 from https://files.pythonhosted.org/packages/a4/f3/8d3a350efb4286b5ebdabcf6736f51d8e3b10dbe68804c6930b00f5cf329/pypdfium2-4.30.1-py3-none-win_amd64.whl.metadata\n",
      "  Downloading pypdfium2-4.30.1-py3-none-win_amd64.whl.metadata (48 kB)\n",
      "     ---------------------------------------- 0.0/48.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 48.2/48.2 kB 2.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from pdfminer.six==20250327->pdfplumber) (2.0.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from pdfminer.six==20250327->pdfplumber) (41.0.3)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\acer\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (2.21)\n",
      "Downloading pdfplumber-0.11.6-py3-none-any.whl (60 kB)\n",
      "   ---------------------------------------- 0.0/60.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 60.2/60.2 kB 1.6 MB/s eta 0:00:00\n",
      "Downloading pdfminer_six-20250327-py3-none-any.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.1/5.6 MB 3.3 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.3/5.6 MB 3.5 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.5/5.6 MB 3.8 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.7/5.6 MB 3.8 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.8/5.6 MB 3.8 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.0/5.6 MB 3.9 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.2/5.6 MB 3.9 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 1.4/5.6 MB 3.9 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 1.6/5.6 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 1.8/5.6 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 1.9/5.6 MB 3.9 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 2.1/5.6 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 2.3/5.6 MB 3.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 2.5/5.6 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 2.7/5.6 MB 3.9 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 2.9/5.6 MB 3.9 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 2.9/5.6 MB 3.9 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.3/5.6 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.4/5.6 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 3.6/5.6 MB 3.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 3.8/5.6 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.0/5.6 MB 3.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.2/5.6 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 4.4/5.6 MB 3.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 4.6/5.6 MB 3.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 4.7/5.6 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 4.9/5.6 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.1/5.6 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.3/5.6 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.5/5.6 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.6/5.6 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.6/5.6 MB 3.9 MB/s eta 0:00:00\n",
      "Downloading pypdfium2-4.30.1-py3-none-win_amd64.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/3.0 MB 4.2 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.3/3.0 MB 4.0 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.5/3.0 MB 3.9 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.7/3.0 MB 3.8 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.8/3.0 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 1.0/3.0 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 1.2/3.0 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.4/3.0 MB 3.8 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.5/3.0 MB 3.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.7/3.0 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.9/3.0 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 2.1/3.0 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 2.3/3.0 MB 3.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 2.4/3.0 MB 3.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.6/3.0 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.8/3.0 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.0/3.0 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.0/3.0 MB 3.7 MB/s eta 0:00:00\n",
      "Installing collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
      "Successfully installed pdfminer.six-20250327 pdfplumber-0.11.6 pypdfium2-4.30.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2 pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46b62af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "جارٍ استخراج النص من Lippincott_Illustrated_Reviews_Pharmacology_7th.pdf ...\n",
      "حدث خطأ أثناء معالجة Lippincott_Illustrated_Reviews_Pharmacology_7th.pdf: [Errno 2] No such file or directory: 'Lippincott_Illustrated_Reviews_Pharmacology_7th.pdf'\n",
      "جارٍ استخراج النص من New-Vital-First-Aid-First-Aid-Book-112019.pdf ...\n",
      "حدث خطأ أثناء معالجة New-Vital-First-Aid-First-Aid-Book-112019.pdf: [Errno 2] No such file or directory: 'New-Vital-First-Aid-First-Aid-Book-112019.pdf'\n",
      "جارٍ استخراج النص من pain_wise_a_patients_guide_to_pain_management_1nbsped_1578264081.pdf ...\n",
      "حدث خطأ أثناء معالجة pain_wise_a_patients_guide_to_pain_management_1nbsped_1578264081.pdf: [Errno 2] No such file or directory: 'pain_wise_a_patients_guide_to_pain_management_1nbsped_1578264081.pdf'\n",
      "✅ تم استخراج جميع النصوص.\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        full_text = ''\n",
    "        for page_num, page in enumerate(reader.pages):\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                full_text += f\"\\n\\n--- PAGE {page_num + 1} ---\\n\\n\"\n",
    "                full_text += text\n",
    "        return full_text\n",
    "\n",
    "books = [\n",
    "    {\"pdf\": \"Lippincott_Illustrated_Reviews_Pharmacology_7th.pdf\", \"txt\": \"data/lippincott_extracted.txt\"},\n",
    "    {\"pdf\": \"New-Vital-First-Aid-First-Aid-Book-112019.pdf\", \"txt\": \"data/first_aid_extracted.txt\"},\n",
    "    {\"pdf\": \"pain_wise_a_patients_guide_to_pain_management_1nbsped_1578264081.pdf\", \"txt\": \"data/pain_management_extracted.txt\"}\n",
    "]\n",
    "\n",
    "for book in books:\n",
    "    try:\n",
    "        print(f\"جارٍ استخراج النص من {book['pdf']} ...\")\n",
    "        pdf_text = extract_text_from_pdf(book['pdf'])\n",
    "        with open(book['txt'], 'w', encoding='utf-8') as f:\n",
    "            f.write(pdf_text)\n",
    "        print(f\"تم الانتهاء من {book['txt']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"حدث خطأ أثناء معالجة {book['pdf']}: {e}\")\n",
    "\n",
    "print(\"✅ تم استخراج جميع النصوص.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f58d7ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ تم تقسيم كتاب الدوائية.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def split_chapters(text, keyword=\"CHAPTER\"):\n",
    "    pattern = rf'({keyword} \\d+[\\s\\S]*?)(?={keyword} \\d+|$)'\n",
    "    chapters_raw = re.findall(pattern, text, re.IGNORECASE)\n",
    "\n",
    "    chapters = []\n",
    "    for i, ch in enumerate(chapters_raw):\n",
    "        title_match = re.search(rf'{keyword} \\d+[^\\\\n]*', ch, re.IGNORECASE)\n",
    "        title = title_match.group(0) if title_match else f\"{keyword} {i + 1}\"\n",
    "        summary = ' '.join(ch.split()[:50]) + \"...\"\n",
    "\n",
    "        chapters.append({\n",
    "            \"chapter_number\": i + 1,\n",
    "            \"title\": title.strip(),\n",
    "            \"summary\": summary.strip(),\n",
    "            \"content\": ch.strip(),\n",
    "            \"keywords\": [],\n",
    "            \"image\": \"\"\n",
    "        })\n",
    "\n",
    "    return chapters\n",
    "\n",
    "with open('data/lippincott_extracted.txt', 'r', encoding='utf-8') as f:\n",
    "    full_text = f.read()\n",
    "\n",
    "chapters = split_chapters(full_text, keyword=\"CHAPTER\")\n",
    "\n",
    "with open('data/lippincott_chapters.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(chapters, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"✅ تم تقسيم كتاب الدوائية.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b135955c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ تم تقسيم كتاب الإسعاف الأولي.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def split_topics(text, keyword=\"Lesson\"):\n",
    "    pattern = rf'({keyword} \\d+[\\s\\S]*?)(?={keyword} \\d+|$)'\n",
    "    topics_raw = re.findall(pattern, text, re.IGNORECASE)\n",
    "\n",
    "    topics = []\n",
    "    for i, tp in enumerate(topics_raw):\n",
    "        title_match = re.search(rf'{keyword} \\d+[^\\\\n]*', tp, re.IGNORECASE)\n",
    "        title = title_match.group(0) if title_match else f\"{keyword} {i + 1}\"\n",
    "        summary = ' '.join(tp.split()[:50]) + \"...\"\n",
    "\n",
    "        topics.append({\n",
    "            \"topic_number\": i + 1,\n",
    "            \"title\": title.strip(),\n",
    "            \"summary\": summary.strip(),\n",
    "            \"content\": tp.strip(),\n",
    "            \"keywords\": [],\n",
    "            \"image\": \"\"\n",
    "        })\n",
    "\n",
    "    return topics\n",
    "\n",
    "with open('data/first_aid_extracted.txt', 'r', encoding='utf-8') as f:\n",
    "    full_text = f.read()\n",
    "\n",
    "topics = split_topics(full_text, keyword=\"Lesson\")\n",
    "\n",
    "with open('data/first_aid_topics.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(topics, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"✅ تم تقسيم كتاب الإسعاف الأولي.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef08653a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ تم تقسيم .\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def split_topics(text, keyword=\"Lesson\"):\n",
    "    pattern = rf'({keyword} \\d+[\\s\\S]*?)(?={keyword} \\d+|$)'\n",
    "    topics_raw = re.findall(pattern, text, re.IGNORECASE)\n",
    "\n",
    "    topics = []\n",
    "    for i, tp in enumerate(topics_raw):\n",
    "        title_match = re.search(rf'{keyword} \\d+[^\\\\n]*', tp, re.IGNORECASE)\n",
    "        title = title_match.group(0) if title_match else f\"{keyword} {i + 1}\"\n",
    "        summary = ' '.join(tp.split()[:50]) + \"...\"\n",
    "\n",
    "        topics.append({\n",
    "            \"topic_number\": i + 1,\n",
    "            \"title\": title.strip(),\n",
    "            \"summary\": summary.strip(),\n",
    "            \"content\": tp.strip(),\n",
    "            \"keywords\": [],\n",
    "            \"image\": \"\"\n",
    "        })\n",
    "\n",
    "    return topics\n",
    "\n",
    "with open('data/pain_management_extracted.txt', 'r', encoding='utf-8') as f:\n",
    "    full_text = f.read()\n",
    "\n",
    "topics = split_topics(full_text, keyword=\"Lesson\")\n",
    "\n",
    "with open('data/pain_management_chapters.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(topics, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"✅ تم تقسيم .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db15de8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9688eac3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c576e060",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42e4ea5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ تم دمج جميع الكتب.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_json_file(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "lippincott_data = load_json_file('data/lippincott_chapters.json')\n",
    "first_aid_data = load_json_file('data/first_aid_topics.json')\n",
    "pain_mgmt_data = load_json_file('data/pain_management_chapters.json')\n",
    "\n",
    "for item in lippincott_data:\n",
    "    item['category'] = 'pharmacology'\n",
    "    item['language'] = 'en'\n",
    "\n",
    "for item in first_aid_data:\n",
    "    item['category'] = 'first_aid'\n",
    "    item['language'] = 'en'\n",
    "\n",
    "for item in pain_mgmt_data:\n",
    "    item['category'] = 'pain_management'\n",
    "    item['language'] = 'en'\n",
    "\n",
    "merged_data = lippincott_data + first_aid_data + pain_mgmt_data\n",
    "\n",
    "with open('data/medical_knowledge.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(merged_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"✅ تم دمج جميع الكتب.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b4e6c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Obtaining dependency information for spacy from https://files.pythonhosted.org/packages/53/29/d4ba96e8c3032f799f778a83356c4956dc5b99cd72d1300704d71e129879/spacy-3.8.5-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached spacy-3.8.5-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Obtaining dependency information for murmurhash<1.1.0,>=0.28.0 from https://files.pythonhosted.org/packages/de/30/ceb9217cdba72bc0bf8466e373e12e5a42945cc85eda0a7c479e319e07ae/murmurhash-1.0.12-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached murmurhash-1.0.12-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Obtaining dependency information for preshed<3.1.0,>=3.0.2 from https://files.pythonhosted.org/packages/e4/fc/78cdbdb79f5d6d45949e72c32445d6c060977ad50a1dcfc0392622165f7c/preshed-3.0.9-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached preshed-3.0.9-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Obtaining dependency information for thinc<8.4.0,>=8.3.4 from https://files.pythonhosted.org/packages/94/40/7e5e840ac2e835fbf5d87e3ab94df7d678d846aaf28b12d46538ed36bf7f/thinc-8.3.6-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached thinc-8.3.6-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Obtaining dependency information for srsly<3.0.0,>=2.4.3 from https://files.pythonhosted.org/packages/bb/da/657a685f63028dcb00ccdc4ac125ed347c8bff6fa0dab6a9eb3dc45f3223/srsly-2.5.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached srsly-2.5.1-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Obtaining dependency information for catalogue<2.1.0,>=2.0.6 from https://files.pythonhosted.org/packages/9e/96/d32b941a501ab566a16358d68b6eb4e4acc373fab3c3c4d7d9e649f7b4bb/catalogue-2.0.10-py3-none-any.whl.metadata\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Obtaining dependency information for weasel<0.5.0,>=0.1.0 from https://files.pythonhosted.org/packages/2a/87/abd57374044e1f627f0a905ac33c1a7daab35a3a815abfea4e1bafd3fdb1/weasel-0.4.1-py3-none-any.whl.metadata\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from spacy) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from spacy) (2.2.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from spacy) (2.10.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\acer\\anaconda3\\lib\\site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from spacy) (24.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Obtaining dependency information for langcodes<4.0.0,>=3.2.0 from https://files.pythonhosted.org/packages/c3/6b/068c2ea7a712bf805c62445bd9e9c06d7340358ef2824150eceac027444b/langcodes-3.5.0-py3-none-any.whl.metadata\n",
      "  Using cached langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Obtaining dependency information for language-data>=1.2 from https://files.pythonhosted.org/packages/5d/e9/5a5ffd9b286db82be70d677d0a91e4d58f7912bb8dd026ddeeb4abe70679/language_data-1.3.0-py3-none-any.whl.metadata\n",
      "  Using cached language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Obtaining dependency information for blis<1.4.0,>=1.3.0 from https://files.pythonhosted.org/packages/35/3a/f9414cf9b2c43aad87e8687ad2cdb0e66e996c20288584621a12725e83dd/blis-1.3.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached blis-1.3.0-cp311-cp311-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Obtaining dependency information for confection<1.0.0,>=0.0.1 from https://files.pythonhosted.org/packages/0c/00/3106b1854b45bd0474ced037dfe6b73b90fe68a68968cef47c23de3d43d2/confection-0.1.5-py3-none-any.whl.metadata\n",
      "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Obtaining dependency information for cloudpathlib<1.0.0,>=0.7.0 from https://files.pythonhosted.org/packages/e8/0f/b1a9b09a84ef98b9fc38d50c6b2815cb2256b804a78e7d838ddfbdc035c7/cloudpathlib-0.21.0-py3-none-any.whl.metadata\n",
      "  Using cached cloudpathlib-0.21.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Obtaining dependency information for marisa-trie>=1.1.0 from https://files.pythonhosted.org/packages/fc/98/574b4e143e0a2f5f71af8716b6c4a8a46220f75a6e0847ce7d11ee0ba4aa/marisa_trie-1.2.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached marisa_trie-1.2.1-cp311-cp311-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Using cached spacy-3.8.5-cp311-cp311-win_amd64.whl (12.2 MB)\n",
      "Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Using cached langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Using cached murmurhash-1.0.12-cp311-cp311-win_amd64.whl (25 kB)\n",
      "Using cached preshed-3.0.9-cp311-cp311-win_amd64.whl (122 kB)\n",
      "Using cached srsly-2.5.1-cp311-cp311-win_amd64.whl (632 kB)\n",
      "Using cached thinc-8.3.6-cp311-cp311-win_amd64.whl (1.8 MB)\n",
      "Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Using cached blis-1.3.0-cp311-cp311-win_amd64.whl (6.2 MB)\n",
      "Using cached cloudpathlib-0.21.0-py3-none-any.whl (52 kB)\n",
      "Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Using cached language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "Using cached marisa_trie-1.2.1-cp311-cp311-win_amd64.whl (152 kB)\n",
      "Installing collected packages: murmurhash, marisa-trie, cloudpathlib, catalogue, blis, srsly, preshed, language-data, langcodes, confection, weasel, thinc, spacy\n",
      "Successfully installed blis-1.3.0 catalogue-2.0.10 cloudpathlib-0.21.0 confection-0.1.5 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 murmurhash-1.0.12 preshed-3.0.9 spacy-3.8.5 srsly-2.5.1 thinc-8.3.6 weasel-0.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script weasel.exe is installed in 'C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script spacy.exe is installed in 'C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install --user spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7fd33f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 393.8 kB/s eta 0:00:33\n",
      "     --------------------------------------- 0.1/12.8 MB 655.4 kB/s eta 0:00:20\n",
      "      --------------------------------------- 0.2/12.8 MB 1.3 MB/s eta 0:00:10\n",
      "     - -------------------------------------- 0.4/12.8 MB 2.0 MB/s eta 0:00:07\n",
      "     - -------------------------------------- 0.6/12.8 MB 2.3 MB/s eta 0:00:06\n",
      "     -- ------------------------------------- 0.8/12.8 MB 2.6 MB/s eta 0:00:05\n",
      "     --- ------------------------------------ 1.0/12.8 MB 2.8 MB/s eta 0:00:05\n",
      "     --- ------------------------------------ 1.2/12.8 MB 2.9 MB/s eta 0:00:05\n",
      "     ---- ----------------------------------- 1.3/12.8 MB 2.9 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 1.5/12.8 MB 3.1 MB/s eta 0:00:04\n",
      "     ----- ---------------------------------- 1.7/12.8 MB 3.1 MB/s eta 0:00:04\n",
      "     ----- ---------------------------------- 1.9/12.8 MB 3.2 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 2.1/12.8 MB 3.2 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 2.3/12.8 MB 3.3 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 2.4/12.8 MB 3.3 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 2.6/12.8 MB 3.3 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 2.8/12.8 MB 3.4 MB/s eta 0:00:03\n",
      "     --------- ------------------------------ 3.0/12.8 MB 3.5 MB/s eta 0:00:03\n",
      "     --------- ------------------------------ 3.1/12.8 MB 3.4 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 3.3/12.8 MB 3.5 MB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 3.5/12.8 MB 3.5 MB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 3.5 MB/s eta 0:00:03\n",
      "     ------------ --------------------------- 3.9/12.8 MB 3.5 MB/s eta 0:00:03\n",
      "     ------------ --------------------------- 4.1/12.8 MB 3.5 MB/s eta 0:00:03\n",
      "     ------------- -------------------------- 4.3/12.8 MB 3.5 MB/s eta 0:00:03\n",
      "     ------------- -------------------------- 4.5/12.8 MB 3.6 MB/s eta 0:00:03\n",
      "     -------------- ------------------------- 4.6/12.8 MB 3.5 MB/s eta 0:00:03\n",
      "     --------------- ------------------------ 4.8/12.8 MB 3.6 MB/s eta 0:00:03\n",
      "     --------------- ------------------------ 5.0/12.8 MB 3.6 MB/s eta 0:00:03\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 3.6 MB/s eta 0:00:03\n",
      "     ---------------- ----------------------- 5.4/12.8 MB 3.6 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 3.6 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 5.7/12.8 MB 3.6 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 5.9/12.8 MB 3.6 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 6.1/12.8 MB 3.7 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 6.2/12.8 MB 3.6 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 6.5/12.8 MB 3.6 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 6.6/12.8 MB 3.7 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 6.8/12.8 MB 3.7 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 7.0/12.8 MB 3.7 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 7.2/12.8 MB 3.7 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 7.4/12.8 MB 3.7 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 7.5/12.8 MB 3.7 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 7.7/12.8 MB 3.7 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 7.9/12.8 MB 3.7 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 8.1/12.8 MB 3.7 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 8.2/12.8 MB 3.7 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 8.4/12.8 MB 3.7 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 8.5/12.8 MB 3.7 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.7/12.8 MB 3.7 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.9/12.8 MB 3.7 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 9.1/12.8 MB 3.7 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 9.3/12.8 MB 3.7 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.5/12.8 MB 3.7 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.6/12.8 MB 3.7 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.8/12.8 MB 3.7 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.0/12.8 MB 3.7 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.2/12.8 MB 3.7 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.4/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.5/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.7/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.9/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.1/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.2/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.4/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.6/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.8/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 11.9/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.1/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.4/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 3.8 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e8b4d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ تم إضافة الكلمات المفتاحية.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def clean_text(text):\n",
    "    return re.sub(r'\\d+|\\W+', ' ', text.lower()).strip()\n",
    "\n",
    "def extract_keywords(text, limit=5):\n",
    "    doc = nlp(clean_text(text))\n",
    "    keywords = [token.text for token in doc if token.pos_ in ['NOUN', 'PROPN', 'ADJ', 'VERB'] and len(token.text) > 3]\n",
    "    return list(set(keywords))[:limit]\n",
    "\n",
    "with open('data/medical_knowledge.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for item in data:\n",
    "    combined = item.get('title', '') + ' ' + item.get('summary', '') + ' ' + item.get('content', '')\n",
    "    item['keywords'] = extract_keywords(combined)\n",
    "\n",
    "with open('data/medical_knowledge_with_keywords.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"✅ تم إضافة الكلمات المفتاحية.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "32b68801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from difflib import get_close_matches\n",
    "\n",
    "with open('data/medical_knowledge_with_keywords.json', 'r', encoding='utf-8') as f:\n",
    "    knowledge_base = json.load(f)\n",
    "\n",
    "def clean_text(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "\n",
    "def find_best_match(question):\n",
    "    question = clean_text(question)\n",
    "    matches = []\n",
    "\n",
    "    for item in knowledge_base:\n",
    "        title = clean_text(item['title'])\n",
    "        summary = clean_text(item.get('summary', ''))\n",
    "        content = clean_text(item.get('content', ''))\n",
    "\n",
    "        if question in title or question in summary or question in content:\n",
    "            matches.append(item)\n",
    "\n",
    "    if not matches:\n",
    "        all_titles = [clean_text(item['title']) for item in knowledge_base]\n",
    "        close_matches = get_close_matches(question, all_titles, n=1, cutoff=0.5)\n",
    "        if close_matches:\n",
    "            best_title = close_matches[0]\n",
    "            for item in knowledge_base:\n",
    "                if clean_text(item['title']) == best_title:\n",
    "                    return item\n",
    "\n",
    "    return matches[0] if matches else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59a75619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d768acfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 01:48:31.658 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-10 01:48:31.901 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\acer\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-05-10 01:48:31.901 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-10 01:48:31.901 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-10 01:48:31.901 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-10 01:48:31.901 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-10 01:48:31.901 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-10 01:48:31.901 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-10 01:48:31.901 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-10 01:48:31.901 Session state does not function when running a script without `streamlit run`\n",
      "2025-05-10 01:48:31.901 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-10 01:48:31.909 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import json\n",
    "import re\n",
    "from difflib import get_close_matches\n",
    "\n",
    "# تحميل قاعدة البيانات\n",
    "with open('data/medical_knowledge_with_keywords.json', 'r', encoding='utf-8') as f:\n",
    "    knowledge_base = json.load(f)\n",
    "\n",
    "def clean_text(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "\n",
    "def find_best_match(question):\n",
    "    question = clean_text(question)\n",
    "    matches = []\n",
    "\n",
    "    for item in knowledge_base:\n",
    "        title = clean_text(item['title'])\n",
    "        summary = clean_text(item.get('summary', ''))\n",
    "        content = clean_text(item.get('content', ''))\n",
    "\n",
    "        if question in title or question in summary or question in content:\n",
    "            matches.append(item)\n",
    "\n",
    "    if not matches:\n",
    "        all_titles = [clean_text(item['title']) for item in knowledge_base]\n",
    "        close_matches = get_close_matches(question, all_titles, n=1, cutoff=0.5)\n",
    "        if close_matches:\n",
    "            best_title = close_matches[0]\n",
    "            for item in knowledge_base:\n",
    "                if clean_text(item['title']) == best_title:\n",
    "                    return item\n",
    "\n",
    "    return matches[0] if matches else None\n",
    "\n",
    "# واجهة Streamlit\n",
    "st.title(\"🤖 الشات بوت الطبي\")\n",
    "st.markdown(\"اكتب سؤالك الطبي وسيقوم البوت بإيجاد المعلومات المناسبة من الكتب.\")\n",
    "\n",
    "user_input = st.text_input(\"اكتب سؤالك هنا...\")\n",
    "\n",
    "if user_input:\n",
    "    result = find_best_match(user_input)\n",
    "    if result:\n",
    "        st.subheader(\"🔍 تم العثور على نتيجة:\")\n",
    "        st.markdown(f\"**العنوان:** {result['title']}\")\n",
    "        st.markdown(f\"**الملخص:** {result.get('summary', 'لا يوجد ملخص')}\")\n",
    "        st.markdown(f\"**المصدر:** {result.get('book', '-')}\")\n",
    "    else:\n",
    "        st.warning(\"عذرًا، لا توجد معلومات متاحة لهذا السؤال.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24422fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5518d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf1fb29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
